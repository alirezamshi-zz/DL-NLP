{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with an LSTM\n",
    "In this notebook, we will train a binary sentiment classifier that is based on an RNN. To this end, will follow these steps:\n",
    "\n",
    "* Preprocess the data: tokenization, stopword removal, lemmatization\n",
    "* Train a word embedding model for reuse in the LSTM\n",
    "* Bring data into format usable for LSTM\n",
    "* Build the RNN architecture\n",
    "* Training the model\n",
    "* Testing the model\n",
    "\n",
    "Finally, there will be a little exercise to make you more comfortable dealing with variable length outputs, which is essential to be able to implement more advanced models later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Deep learning models are known to be able to require little feature engineering, and this is also true for SOTA models in NLP. However, sometimes, especially on low-resource tasks, it may be better to do some basic preprocessing to the text instead of passing the raw stream of text. By doing so, we hard-code linguistic knowledge that makes it easier for the neural network to train.\n",
    "\n",
    "But first, lets load the data (a small sample so things don't take forever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_data(num_data=25000):\n",
    "    reviewsFile = open('../data/reviews.txt','r')\n",
    "    reviews = list(map(lambda x:x[:-1],reviewsFile.readlines()))\n",
    "    reviewsFile.close()\n",
    "\n",
    "    labelsFile = open('../data/labels.txt','r')\n",
    "    labels = list(map(lambda x:x[:-1],labelsFile.readlines()))\n",
    "    labelsFile.close()\n",
    "    \n",
    "    reviews, labels = reviews[:num_data], labels[:num_data]\n",
    "    \n",
    "    return reviews,labels\n",
    "\n",
    "reviews,labels = load_data(num_data=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "In theory, we could feed the neural network model the input text one character at a time, and there are in fact models that do so and work well.\n",
    "However, this approach ignores any prior linguistic knowledge, even the knowledge of what constitutes a word. \"Tokenization\" means to split up the text into tokens, i.e., the smallest units to be considered by subsequent steps.\n",
    "\n",
    "Here, we use regular expressions to identify words that may or may not contain a \"'\", e.g., \"didn't\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"\\w+\\'?\\w+|\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_token(review):\n",
    "    return tokenizer.tokenize(str(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word removal\n",
    "For some applications, certain words are not very useful for the task at hand because they provide no or only little indication of the task's semantics. These words are called stopwords. For (topical) text classification, these often include common words such as articles, conjunctions, and pronouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for sentiment classification many of these words can be useful because they can have a large impact on the actual sentiment expressed in the sentence (e.g., not). Hence, we don't want to exclude them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptionStopWords = {\n",
    "    'again',\n",
    "    'against',\n",
    "    'ain',\n",
    "    'almost',\n",
    "    'among',\n",
    "    'amongst',\n",
    "    'amount',\n",
    "    'anyhow',\n",
    "    'anyway',\n",
    "    'aren',\n",
    "    \"aren't\",\n",
    "    'below',\n",
    "    'bottom',\n",
    "    'but',\n",
    "    'cannot',\n",
    "    'couldn',\n",
    "    \"couldn't\",\n",
    "    'didn',\n",
    "    \"didn't\",\n",
    "    'doesn',\n",
    "    \"doesn't\",\n",
    "    'don',\n",
    "    \"don't\",\n",
    "    'done',\n",
    "    'down',\n",
    "    'except',\n",
    "    'few',\n",
    "    'hadn',\n",
    "    \"hadn't\",\n",
    "    'hasn',\n",
    "    \"hasn't\",\n",
    "    'haven',\n",
    "    \"haven't\",\n",
    "    'however',\n",
    "    'isn',\n",
    "    \"isn't\",\n",
    "    'least',\n",
    "    'mightn',\n",
    "    \"mightn't\",\n",
    "    'move',\n",
    "    'much',\n",
    "    'must',\n",
    "    'mustn',\n",
    "    \"mustn't\",\n",
    "    'needn',\n",
    "    \"needn't\",\n",
    "    'neither',\n",
    "    'never',\n",
    "    'nevertheless',\n",
    "    'no',\n",
    "    'nobody',\n",
    "    'none',\n",
    "    'noone',\n",
    "    'nor',\n",
    "    'not',\n",
    "    'nothing',\n",
    "    'should',\n",
    "    \"should've\",\n",
    "    'shouldn',\n",
    "    \"shouldn't\",\n",
    "    'too',\n",
    "    'top',\n",
    "    'up',\n",
    "    'very'\n",
    "    'wasn',\n",
    "    \"wasn't\",\n",
    "    'well',\n",
    "    'weren',\n",
    "    \"weren't\",\n",
    "    'won',\n",
    "    \"won't\",\n",
    "    'wouldn',\n",
    "    \"wouldn't\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stop_words).union(STOP_WORDS)\n",
    "\n",
    "final_stop_words = stop_words-exceptionStopWords\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    return [token for token in review if token not in final_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Finally, lemmatization is a technique to reduce the number of words considered by the model by reducing different inflections of a word to the root. The 'spacy' library provides convenient functionality for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en\",disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(review):\n",
    "    lemma_result = []\n",
    "    \n",
    "    for words in review:\n",
    "        doc = nlp(words)\n",
    "        for token in doc:\n",
    "            lemma_result.append(token.lemma_)\n",
    "    return lemma_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can preprocess all reviews as a pipeline of tokenization, stopword removal, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(review):\n",
    "    review = make_token(review)\n",
    "    review = remove_stopwords(review)\n",
    "    return lemmatization(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min, sys: 222 ms, total: 4min\n",
      "Wall time: 4min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reviews = list(map(lambda review: pipeline(review),reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bromwell',\n",
       "  'high',\n",
       "  'cartoon',\n",
       "  'comedy',\n",
       "  'run',\n",
       "  'time',\n",
       "  'program',\n",
       "  'school',\n",
       "  'life',\n",
       "  'teacher',\n",
       "  'year',\n",
       "  'teach',\n",
       "  'profession',\n",
       "  'lead',\n",
       "  'believe',\n",
       "  'bromwell',\n",
       "  'high',\n",
       "  'satire',\n",
       "  'much',\n",
       "  'close',\n",
       "  'reality',\n",
       "  'teacher',\n",
       "  'scramble',\n",
       "  'survive',\n",
       "  'financially',\n",
       "  'insightful',\n",
       "  'student',\n",
       "  'right',\n",
       "  'pathetic',\n",
       "  'teacher',\n",
       "  'pomp',\n",
       "  'pettiness',\n",
       "  'situation',\n",
       "  'remind',\n",
       "  'school',\n",
       "  'know',\n",
       "  'student',\n",
       "  'see',\n",
       "  'episode',\n",
       "  'student',\n",
       "  'repeatedly',\n",
       "  'try',\n",
       "  'burn',\n",
       "  'down',\n",
       "  'school',\n",
       "  'immediately',\n",
       "  'recall',\n",
       "  'high',\n",
       "  'classic',\n",
       "  'line',\n",
       "  'inspector',\n",
       "  'sack',\n",
       "  'teacher',\n",
       "  'student',\n",
       "  'welcome',\n",
       "  'bromwell',\n",
       "  'high',\n",
       "  'expect',\n",
       "  'adult',\n",
       "  'age',\n",
       "  'think',\n",
       "  'bromwell',\n",
       "  'high',\n",
       "  'far',\n",
       "  'fetch',\n",
       "  'pity',\n",
       "  'isn'],\n",
       " ['story',\n",
       "  'man',\n",
       "  'unnatural',\n",
       "  'feeling',\n",
       "  'pig',\n",
       "  'start',\n",
       "  'open',\n",
       "  'scene',\n",
       "  'terrific',\n",
       "  'example',\n",
       "  'absurd',\n",
       "  'comedy',\n",
       "  'formal',\n",
       "  'orchestra',\n",
       "  'audience',\n",
       "  'turn',\n",
       "  'insane',\n",
       "  'violent',\n",
       "  'mob',\n",
       "  'crazy',\n",
       "  'chantings',\n",
       "  'singer',\n",
       "  'unfortunately',\n",
       "  'stay',\n",
       "  'absurd',\n",
       "  'time',\n",
       "  'no',\n",
       "  'general',\n",
       "  'narrative',\n",
       "  'eventually',\n",
       "  'make',\n",
       "  'too',\n",
       "  'putt',\n",
       "  'era',\n",
       "  'should',\n",
       "  'turn',\n",
       "  'cryptic',\n",
       "  'dialogue',\n",
       "  'shakespeare',\n",
       "  'easy',\n",
       "  'grader',\n",
       "  'technical',\n",
       "  'level',\n",
       "  'well',\n",
       "  'think',\n",
       "  'good',\n",
       "  'cinematography',\n",
       "  'future',\n",
       "  'great',\n",
       "  'vilmos',\n",
       "  'zsigmond',\n",
       "  'future',\n",
       "  'star',\n",
       "  'sally',\n",
       "  'kirkland',\n",
       "  'frederic',\n",
       "  'forrest',\n",
       "  'see',\n",
       "  'briefly']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a word2vec model\n",
    "As you very well know, the neural networks we consider should not be fed with high-dimensional one-hot-vectors directly. Instead, it is better to map them into low-dimensional space first (word embeddings). It is possible to just randomly initialize word embedding tables and train them from scratch jointly with the sentiment classification task. However, it often better to initialize the word embedding tables with pretrained word embeddings and finetune them on the task afterwards. It is common to use freely available word embeddings pretrained on large corpora, but here we train one from scratch.\n",
    "\n",
    "With the gensim library, this is as simple as calling the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_dimension = 100\n",
    "model = Word2Vec(reviews,size=embedding_dimension, window=3, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28163"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    "del model\n",
    "\n",
    "len(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When inspecting the nearest neighbors of some of the words important to sentiment analysis, we can see that the word embeddings already capture some useful semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.7292054891586304),\n",
       " ('alright', 0.6922928094863892),\n",
       " ('okay', 0.6738833785057068),\n",
       " ('nice', 0.6543942093849182),\n",
       " ('great', 0.6427708864212036)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(word=\"good\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrible', 0.7072439193725586),\n",
       " ('suck', 0.7059657573699951),\n",
       " ('terrible', 0.6890344023704529),\n",
       " ('awful', 0.6877450942993164),\n",
       " ('lame', 0.6699240803718567)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(word=\"bad\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrible', 0.7072439193725586),\n",
       " ('suck', 0.7059657573699951),\n",
       " ('terrible', 0.6890344023704529),\n",
       " ('awful', 0.6877450942993164)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=\"bad\",topn=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5836363"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity(\"good\",\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.328366"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity(\"good\",\"be\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('college', 0.7752484679222107),\n",
       " ('class', 0.7588450908660889),\n",
       " ('schooler', 0.7476534843444824),\n",
       " ('bromwell', 0.7275527715682983),\n",
       " ('bidder', 0.706292450428009)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(word=\"school\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('slapstick', 0.6493667960166931),\n",
       " ('farce', 0.6451849937438965),\n",
       " ('humor', 0.6442668437957764),\n",
       " ('satire', 0.6236644983291626),\n",
       " ('humour', 0.6191367506980896)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(word=\"comedy\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('suspense', 0.6244109869003296),\n",
       " ('thrill', 0.6063871383666992),\n",
       " ('gory', 0.597480058670044),\n",
       " ('courtroom', 0.5860505104064941),\n",
       " ('fantasy', 0.5682624578475952)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(word=\"action\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('depress', 0.7488235831260681),\n",
       " ('cry', 0.7024545669555664),\n",
       " ('heartwarming', 0.6907929182052612),\n",
       " ('genuinely', 0.679267168045044),\n",
       " ('honest', 0.6713963747024536)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(word=\"sad\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('solid', 0.3986845016479492),\n",
       " ('fine', 0.3981825113296509),\n",
       " ('splendid', 0.360782265663147),\n",
       " ('incredible', 0.3579597771167755),\n",
       " ('fantastic', 0.35041236877441406)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(negative=[\"bad\"],positive=[\"decent\"],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_value = len(word_vectors.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28163"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for neural networks\n",
    "As noted before, we want to reuse the trained embeddings in our network later, so we reuse the word->index mapping from the word2vec model and turn them into pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix seeds for pytorch for reproducibility\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "SEED = 2222\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to map all words in a review into a tensor of indices.\n",
    "def word2idx(embedding_model,review):\n",
    "    index_review = []\n",
    "    for word in review:\n",
    "        try:\n",
    "            index_review.append(embedding_model.vocab[word].index)\n",
    "        except: \n",
    "             pass\n",
    "    return torch.tensor(index_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_review = list(map(lambda review: word2idx(word_vectors,review),reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, lets split the data into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 5000 4000\n",
      "16000 5000 4000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "labels = [0 if label == 'negative' else 1 for label in labels ]\n",
    "X_train, X_test, y_train, y_test = train_test_split(index_review, labels, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "print(len(X_train),len(X_test),len(X_val))\n",
    "print(len(y_train),len(y_test),len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with batches\n",
    "Text data is different insofar as that it is variable length. This makes processing data in batches relatively complicated, however, parallelization via batching is crucial to train the neural networks in reasonable time. To cope with this problem, we need to pad all sentences in a batch to equal length. Below, we pad all sequences to the length of the longest sequence, however, sometimes it is reasonable to put a hard cap on the sequence length to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def iterator_func(X,y):\n",
    "    size = len(X)\n",
    "    permutation = np.random.permutation(size)\n",
    "    iterator = []\n",
    "    for i in range(0,size, batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch = {}\n",
    "        batch[\"text\"] = [X[i] for i in indices]\n",
    "        batch[\"label\"] = [y[i] for i in indices]\n",
    "        \n",
    "        batch[\"text\"],batch[\"label\"] = zip(*sorted(zip(batch[\"text\"],batch[\"label\"]),key=lambda x: len(x[0]),reverse=True))\n",
    "        batch[\"length\"] = [len(review) for review in batch[\"text\"]]\n",
    "        batch[\"length\"] = torch.IntTensor(batch[\"length\"])\n",
    "        batch[\"text\"] = torch.nn.utils.rnn.pad_sequence(batch[\"text\"],batch_first=True).t() # pads all sequences max length\n",
    "        batch[\"label\"] = torch.Tensor(batch[\"label\"])\n",
    "        \n",
    "        batch[\"label\"]  = batch[\"label\"].to(device)\n",
    "        batch[\"length\"] = batch[\"length\"].to(device) \n",
    "        batch[\"text\"]   = batch[\"text\"].to(device) \n",
    "        \n",
    "        iterator.append(batch)\n",
    "        \n",
    "    return iterator\n",
    "\n",
    "train_iterator = iterator_func(X_train,y_train)\n",
    "valid_iterator = iterator_func(X_val,y_val)\n",
    "test_iterator = iterator_func(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN\n",
    "For the implementation of the RNN, we provide two implementations. The first one is an explicit implementation of the procedure as it was introduced to you in the lecture: At each step, the RNN (or, LSTM in this particular case) takes the current hidden state and the element at the current timestep as input and produces an output as well as a new hidden state. This is implemented as a for loop. Because we are dealing with variable length sequences which have been padded to fit a batch, we cannot just take the output of the last hidden state as the representation of the sequence, because it will incorrectly consider the pad tokens as a valid input. Therefore, we need to extract the output at the last _valid_ output, which will be used to represent the text. This representation is input to a logistic regression classifier to get the final binary output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNExplicit(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, embedding_weights):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, text_lengths):\n",
    "        #x [sent length , batch size]\n",
    "        embedded = self.embedding(x) #[sentect len,batch size,embedding dim]\n",
    "        max_len = embedded.size(0)\n",
    "        batch_size = embedded.size(1)\n",
    "        hidden_states = (torch.zeros((1, batch_size, self.hidden_dim)), torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        output = []\n",
    "        for t in range(max_len):\n",
    "            embedded_t = embedded[t].unsqueeze(0)\n",
    "            out, (hidden, cell) = self.rnn(embedded_t, hidden_states)#output[sent length,batch size,hiddendin*num of directions],[numberlayers*num of dir,batch size,hid dim]\n",
    "            hidden_states = (hidden, cell)\n",
    "            output.append(out)\n",
    "        #[f0,b0,f1,b1,.......fn,bn]\n",
    "        output = torch.cat(output, dim = 0)\n",
    "        \n",
    "        # get the last output\n",
    "        text_lengths = torch.tensor(text_lengths, requires_grad=False).long()\n",
    "        masks = (text_lengths - 1).unsqueeze(0).unsqueeze(2)\n",
    "        masks = masks.expand(max_len, -1, self.hidden_dim)\n",
    "        output = output.gather(0, masks)[0, :, :]\n",
    "        \n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the above code is both slow and inconvenient and should thus not be used in practice. PyTorch provides convenient functionality that makes dealing with variable length sequences easier and faster. The below code makes use of the PackedSequence class, which can be passed directly to RNNs in one go. At the end, the 'hidden' variable will automatically contain the outputs at the last _valid_ timestep for each element of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, embedding_weights):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, text_lengths):\n",
    "        #x [sent length , batch size]\n",
    "        embedded = self.embedding(x) #[sentect len,batch size,embedding dim]\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)#output[sent length,batch size,hiddendin*num of directions],[numberlayers*num of dir,batch size,hid dim]\n",
    "\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Lastly, it remains to train the model we just created. Nothing new happens here - we just need to configure the model and training parameters, set up the training loop, and run the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(28163, 100)\n",
       "  (rnn): LSTM(100, 100)\n",
       "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = padding_value\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "N_EPOCHS = 5\n",
    "embedding_weights = torch.Tensor(word_vectors.vectors)\n",
    "\n",
    "#model = RNNExplicit(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, embedding_weights)\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, embedding_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch[\"text\"],batch[\"length\"]).squeeze(1)\n",
    "        loss = criterion(predictions, batch[\"label\"])\n",
    "        acc = binary_accuracy(predictions, batch[\"label\"])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch[\"text\"],batch[\"length\"]).squeeze(1)\n",
    "            loss = criterion(predictions, batch[\"label\"])\n",
    "            acc = binary_accuracy(predictions, batch[\"label\"])\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.507 | Train Acc: 75.31% | Val. Loss: 0.418 | Val. Acc: 81.62% |\n",
      "| Epoch: 02 | Train Loss: 0.422 | Train Acc: 81.47% | Val. Loss: 0.414 | Val. Acc: 83.13% |\n",
      "| Epoch: 03 | Train Loss: 0.379 | Train Acc: 84.12% | Val. Loss: 0.386 | Val. Acc: 83.11% |\n",
      "| Epoch: 04 | Train Loss: 0.357 | Train Acc: 84.72% | Val. Loss: 0.367 | Val. Acc: 84.30% |\n",
      "| Epoch: 05 | Train Loss: 0.334 | Train Acc: 86.08% | Val. Loss: 0.341 | Val. Acc: 85.69% |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.348 | Test Acc: 85.20% |\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sentence):\n",
    "    tokenized = pipeline(sentence)\n",
    "    indexed = word2idx(word_vectors,tokenized)\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor,torch.LongTensor([len(indexed)]).to(device)))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6866046786308289"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"this is an awesome movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7844770550727844"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"this is not an action movie, is a  very good movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03949751332402229"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"Despite the terrible title, the bad credits section in the end,\"\n",
    "                  \n",
    "                  \" and the low-quality sounds here\"\\\n",
    "                  \" and there, this is a movie of extraordinary quality.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11702029407024384"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"this is an awful movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19338952004909515"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"this is a bad movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "In the model above, we used the hidden state at the last timestep to represent the whole sequence. But often this is not the best way to aggregate the outputs of an RNN: Average or max pooling often works better. In the following, we ask you to implement and test mean- and max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bd289438953c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mOtherRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"max\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class OtherRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, embedding_weights, aggregation = \"max\"):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.aggregation = aggregation\n",
    "        \n",
    "    def forward(self, x, text_lengths):\n",
    "        #x [sent length , batch size]\n",
    "        embedded = self.embedding(x) #[sentect len,batch size,embedding dim]\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)#output[sent length,batch size,hiddendin],[1,batch size,hid dim]\n",
    "        outputs, output_lens = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=False)\n",
    "\n",
    "        # TODO: add your code here!\n",
    "        if self.aggregation == \"max\":\n",
    "            #sequence_embedding = ...\n",
    "            pass\n",
    "        elif self.aggregation == \"mean\":\n",
    "            #sequence_embedding = ...\n",
    "            pass\n",
    "        \n",
    "        return self.fc(sequence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
